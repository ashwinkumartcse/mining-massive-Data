{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes_PySpark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNmDZVMW_DYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "7610625c-5ef0-4c7b-e2f8-d108fee99f79"
      },
      "source": [
        "# Installation of pyspark \n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 61kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=d3e0c7270b4357a87267a5be7cd99eb53ddcebf8841ff04f1072ef29bdbe6c2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 35.8 MB of archives.\n",
            "After this operation, 140 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 144429 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u252-b09-1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u252-b09-1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u252-b09-1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u252-b09-1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u252-b09-1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u252-b09-1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUqGAf7d4CD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "sqlContext = SQLContext(sc)\n",
        "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/content/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNbTptyzTzJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a6617641-0eaf-439c-a2f9-86f16159dda9"
      },
      "source": [
        "drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
        "data = data.select([column for column in data.columns if column not in drop_list])\n",
        "data.show(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+\n",
            "|      Category|            Descript|\n",
            "+--------------+--------------------+\n",
            "|      WARRANTS|      WARRANT ARREST|\n",
            "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
            "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
            "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
            "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
            "+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tErzKO1oT5fy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "20371c11-4028-47f0-9a0b-8cb670c4cb91"
      },
      "source": [
        "data.printSchema()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Descript: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNO0-r5lT7in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "cab960c6-b5c9-4af8-8413-595f816701c0"
      },
      "source": [
        "#Top 20 crime categories:\n",
        "from pyspark.sql.functions import col\n",
        "data.groupBy(\"Category\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|            Category| count|\n",
            "+--------------------+------+\n",
            "|       LARCENY/THEFT|174900|\n",
            "|      OTHER OFFENSES|126182|\n",
            "|        NON-CRIMINAL| 92304|\n",
            "|             ASSAULT| 76876|\n",
            "|       DRUG/NARCOTIC| 53971|\n",
            "|       VEHICLE THEFT| 53781|\n",
            "|           VANDALISM| 44725|\n",
            "|            WARRANTS| 42214|\n",
            "|            BURGLARY| 36755|\n",
            "|      SUSPICIOUS OCC| 31414|\n",
            "|      MISSING PERSON| 25989|\n",
            "|             ROBBERY| 23000|\n",
            "|               FRAUD| 16679|\n",
            "|FORGERY/COUNTERFE...| 10609|\n",
            "|     SECONDARY CODES|  9985|\n",
            "|         WEAPON LAWS|  8555|\n",
            "|        PROSTITUTION|  7484|\n",
            "|            TRESPASS|  7326|\n",
            "|     STOLEN PROPERTY|  4540|\n",
            "|SEX OFFENSES FORC...|  4388|\n",
            "+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_vJG_lGUCsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "25b27236-fdc0-42b1-ded4-602606015f07"
      },
      "source": [
        "#Top 20 crime descriptions:\n",
        "data.groupBy(\"Descript\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            Descript|count|\n",
            "+--------------------+-----+\n",
            "|GRAND THEFT FROM ...|60022|\n",
            "|       LOST PROPERTY|31729|\n",
            "|             BATTERY|27441|\n",
            "|   STOLEN AUTOMOBILE|26897|\n",
            "|DRIVERS LICENSE, ...|26839|\n",
            "|      WARRANT ARREST|23754|\n",
            "|SUSPICIOUS OCCURR...|21891|\n",
            "|AIDED CASE, MENTA...|21497|\n",
            "|PETTY THEFT FROM ...|19771|\n",
            "|MALICIOUS MISCHIE...|17789|\n",
            "|   TRAFFIC VIOLATION|16471|\n",
            "|PETTY THEFT OF PR...|16196|\n",
            "|MALICIOUS MISCHIE...|15957|\n",
            "|THREATS AGAINST LIFE|14716|\n",
            "|      FOUND PROPERTY|12146|\n",
            "|ENROUTE TO OUTSID...|11470|\n",
            "|GRAND THEFT OF PR...|11010|\n",
            "|POSSESSION OF NAR...|10050|\n",
            "|PETTY THEFT FROM ...|10029|\n",
            "|PETTY THEFT SHOPL...| 9571|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPqjTni8UKIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "# regular expression tokenizer\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "# stop words\n",
        "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
        "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
        "# bag of words count\n",
        "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LInZkCIMVyvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a02bc05f-f2d3-4c80-db27-143722996804"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "dataset.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "|      Category|            Descript|               words|            filtered|            features|label|\n",
            "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "|      WARRANTS|      WARRANT ARREST|   [warrant, arrest]|   [warrant, arrest]|(809,[17,32],[1.0...|  7.0|\n",
            "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
            "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
            "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
            "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
            "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZwhrncLUmvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "751aa406-3bd7-46cb-cffd-ae2d2faf42f4"
      },
      "source": [
        "# set seed for reproducibility\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 613959\n",
            "Test Dataset Count: 264090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8HLeHi0U-eg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "e1da0660-01e5-4500-8ee1-cc1e05fd56ed"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb = NaiveBayes(smoothing=1)\n",
        "model = nb.fit(trainingData)\n",
        "predictions = model.transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "|                    Descript|     Category|                   probability|label|prediction|\n",
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999987111,1.452774...|  0.0|       0.0|\n",
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca7MgBOFWeBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2473a183-0d8d-4af7-c222-9c181ecec4bf"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9939437157625493"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}